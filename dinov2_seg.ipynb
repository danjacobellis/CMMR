{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1569f7a3-3ea5-4003-8997-62a3971dd8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnf\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torchvision import transforms\n",
    "from torchmetrics.classification import MulticlassJaccardIndex\n",
    "import numpy as np\n",
    "from transformers import AutoModel\n",
    "from compressai.models import CompressionModel\n",
    "from compressai.entropy_models import EntropyBottleneck\n",
    "from compressai.layers import GDN1\n",
    "import zlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c977a2e4-82bf-4613-955f-c0ba7e4d2652",
   "metadata": {},
   "outputs": [],
   "source": [
    "ade20k = load_dataset(\"scene_parse_150\",split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "737ffac6-4878-4115-b39f-6d466c8f5055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSegmentationHead(nn.Module):\n",
    "    def __init__(self, in_channels=1536, num_classes=150):\n",
    "        super(LinearSegmentationHead, self).__init__()\n",
    "        self.conv_seg = nn.Conv2d(in_channels, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_seg(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c5f3424-bab3-425b-b0fa-46a8e7c114eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained('facebook/dinov2-giant')\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de3347a2-b4a1-47d5-a732-1f966b370253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(in_channels, out_channels, kernel_size=5, stride=2, groups=32):\n",
    "    return nn.Conv2d(\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=kernel_size,\n",
    "        stride=stride,\n",
    "        padding=kernel_size // 2,\n",
    "        groups=groups\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f81ebab8-4841-4e38-a46b-148a89544c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv(in_channels, out_channels, kernel_size=5, stride=2, groups=32):\n",
    "    return nn.ConvTranspose2d(\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=kernel_size,\n",
    "        stride=stride,\n",
    "        output_padding=stride - 1,\n",
    "        padding=kernel_size // 2,\n",
    "        groups=groups\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd2ec69e-7b8c-4c5a-a2a6-6f587124fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateDistortionAutoEncoder(CompressionModel):\n",
    "    def __init__(self, N=4096):\n",
    "        super().__init__()\n",
    "        self.entropy_bottleneck = EntropyBottleneck(N)\n",
    "        self.encode = nn.Sequential(\n",
    "            conv(1536, N, kernel_size=1, stride=1),\n",
    "            GDN1(N),\n",
    "            conv(N, N, kernel_size=5, stride=2),\n",
    "            GDN1(N),\n",
    "            conv(N, N, kernel_size=5, stride=2),\n",
    "        )\n",
    "\n",
    "        self.decode = nn.Sequential(\n",
    "            deconv(N, N, kernel_size=5, stride=2),\n",
    "            GDN1(N, inverse=True),\n",
    "            deconv(N, N, kernel_size=5, stride=2),\n",
    "            GDN1(N, inverse=True),\n",
    "            deconv(N, 1536, kernel_size=1, stride=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.encode(x)\n",
    "        y_hat, y_likelihoods = self.entropy_bottleneck(y)\n",
    "        x_hat = self.decode(y_hat)\n",
    "        return x_hat, y_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d81d492e-2b1b-4618-99d7-42872a1538b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_checkpoint = torch.load(\"dinov2_vitg14_ade20k_linear_head.pth\")\n",
    "new_state_dict = {key.replace(\"decode_head.\", \"\"): value for key, value in head_checkpoint['state_dict'].items()}\n",
    "new_state_dict.pop('bn.weight', None)\n",
    "new_state_dict.pop('bn.bias', None)\n",
    "new_state_dict.pop('bn.running_var', None)\n",
    "new_state_dict.pop('bn.running_mean', None)\n",
    "new_state_dict.pop('bn.num_batches_tracked', None)\n",
    "head = LinearSegmentationHead()\n",
    "head.load_state_dict(new_state_dict)\n",
    "head = head.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfcb21e3-0e88-48c6-8797-8cf053445517",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdae_checkpoint = torch.load(\"checkpoint_dino_rdae_20.pth\", map_location={'cuda:0': 'cpu'})\n",
    "rdae = RateDistortionAutoEncoder()\n",
    "rdae.load_state_dict(rdae_checkpoint['model_state_dict'])\n",
    "rdae = rdae.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09de4929-f731-4a33-a2c6-e8c584d94a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossy_compress_patch_tokens(x):\n",
    "    with torch.no_grad():\n",
    "        z = rdae.encode(x)\n",
    "        z = z.round()\n",
    "        z = z.clamp(-128,127)\n",
    "        z = z.to(torch.int8)\n",
    "        z = z.to(\"cpu\").detach().numpy()\n",
    "        original_shape = z.shape\n",
    "        compressed = zlib.compress(z.tobytes(), level=9)\n",
    "        decompressed = zlib.decompress(compressed)\n",
    "        ẑ = np.frombuffer(decompressed, dtype=np.int8)\n",
    "        ẑ = ẑ.reshape(original_shape)\n",
    "        ẑ = torch.tensor(ẑ)\n",
    "        ẑ = ẑ.to(torch.float).to(\"cuda\")\n",
    "        x̂ = rdae.decode(ẑ)\n",
    "        bps = 8*len(compressed)/(1536*32*18)\n",
    "        return x̂,bps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf9bb368-30f7-49ca-ad6b-ced1cf55384d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 49s, sys: 1.11 s, total: 6min 51s\n",
      "Wall time: 6min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "iou = []\n",
    "bps = []\n",
    "for i_sample,sample in enumerate(ade20k):\n",
    "    img = sample['image'].resize((448,252))\n",
    "    ground_truth = sample['annotation']\n",
    "    while ground_truth.width > 1000:\n",
    "        ground_truth = ground_truth.resize((ground_truth.width//2,ground_truth.height//2),PIL.Image.Resampling.NEAREST)\n",
    "    x = transforms.ToTensor()(img).unsqueeze(0).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        y = model.forward(x)[0]\n",
    "        cls_token = y[:, 0]\n",
    "        patch_tokens = y[:, 1:]\n",
    "        patch_tokens = patch_tokens.reshape((1,18,32,1536)).permute((0,3,1,2))\n",
    "\n",
    "        patch_tokens,bps_i = lossy_compress_patch_tokens(patch_tokens)\n",
    "        bps.append(bps_i)\n",
    "        \n",
    "        patch_tokens = nnf.interpolate(patch_tokens,\n",
    "                                       size=(ground_truth.height,ground_truth.width),\n",
    "                                       mode='bilinear')\n",
    "        logits = head(patch_tokens)\n",
    "        predicted = ToPILImage()(logits[0].argmax(dim=0).to(torch.uint8))\n",
    "        \n",
    "        x1 = transforms.PILToTensor()(ground_truth) \n",
    "        x2 = transforms.PILToTensor()(predicted)\n",
    "        x1 = x1-1\n",
    "\n",
    "        iou.append(\n",
    "            MulticlassJaccardIndex(num_classes=151,average='micro',ignore_index=255)\n",
    "            (x1,x2).item()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46ad5a22-e21c-431b-aa43-51ff56112d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3364191932779795"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff5d3861-c1ad-42ac-b69e-4672f5799296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.31396681539592"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32/np.mean(bps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
