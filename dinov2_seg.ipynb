{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1569f7a3-3ea5-4003-8997-62a3971dd8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnf\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torchvision import transforms\n",
    "from torchmetrics.classification import MulticlassJaccardIndex\n",
    "import numpy as np\n",
    "from transformers import AutoModelForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d84cb093-e1fe-4b05-bc32-8cf4e37f5516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868eeb367ebf4c6193dc654dac87c95e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0176f3bd4cb041a1ba7d0c13bcf5a3ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ade20k_dino = load_dataset(\"danjacobellis/ade20k_dino\",split='validation').with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c977a2e4-82bf-4613-955f-c0ba7e4d2652",
   "metadata": {},
   "outputs": [],
   "source": [
    "ade20k = load_dataset(\"scene_parse_150\",split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c5f3424-bab3-425b-b0fa-46a8e7c114eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Dinov2ForImageClassification were not initialized from the model checkpoint at facebook/dinov2-giant and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForImageClassification.from_pretrained('facebook/dinov2-giant')\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee793336-b7c0-4ca0-a07b-5cadffba40f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14').to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "737ffac6-4878-4115-b39f-6d466c8f5055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSegmentationHead(nn.Module):\n",
    "    def __init__(self, in_channels=1536, num_classes=150):\n",
    "        super(LinearSegmentationHead, self).__init__()\n",
    "        self.conv_seg = nn.Conv2d(in_channels, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_seg(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7dd914d-fc39-406a-a005-42ddc0163105",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"dinov2_vitg14_ade20k_linear_head.pth\")\n",
    "new_state_dict = {key.replace(\"decode_head.\", \"\"): value for key, value in checkpoint['state_dict'].items()}\n",
    "new_state_dict.pop('bn.weight', None)\n",
    "new_state_dict.pop('bn.bias', None)\n",
    "new_state_dict.pop('bn.running_var', None)\n",
    "new_state_dict.pop('bn.running_mean', None)\n",
    "new_state_dict.pop('bn.num_batches_tracked', None)\n",
    "head = LinearSegmentationHead()\n",
    "head.load_state_dict(new_state_dict)\n",
    "head = head.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d432687-6746-4654-a6c3-633e21acb926",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 38s, sys: 870 ms, total: 3min 39s\n",
      "Wall time: 2min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "iou = []\n",
    "for sample in ade20k:\n",
    "    img = sample['image'].resize((224,224))\n",
    "    ground_truth = sample['annotation']\n",
    "    while ground_truth.width > 1000:\n",
    "        ground_truth = ground_truth.resize((ground_truth.width//2,ground_truth.height//2),PIL.Image.Resampling.NEAREST)\n",
    "    x = transforms.ToTensor()(img).unsqueeze(0).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        y = model.dinov2.forward(x)[0]\n",
    "        # y = model.forward_features(x)\n",
    "        \n",
    "        cls_token = y[:, 0].detach()\n",
    "        patch_tokens = y[:, 1:].detach()\n",
    "        # cls_token = y['x_norm_clstoken']\n",
    "        # patch_tokens = y['x_norm_patchtokens']\n",
    "        \n",
    "        patch_tokens = patch_tokens.reshape((1,16,16,1536)).permute((0,3,1,2))\n",
    "        patch_tokens = nnf.interpolate(patch_tokens,\n",
    "                                       size=(ground_truth.height,ground_truth.width),\n",
    "                                       mode='bilinear'\n",
    "                                      )\n",
    "        logits = head(patch_tokens)\n",
    "        predicted = ToPILImage()(logits[0].argmax(dim=0).to(torch.uint8))\n",
    "        # predicted = predicted.resize((ground_truth.width,ground_truth.height),resample=PIL.Image.Resampling.BILINEAR)\n",
    "    \n",
    "        x1 = transforms.PILToTensor()(ground_truth) \n",
    "        x2 = transforms.PILToTensor()(predicted)\n",
    "        x1 = x1-1\n",
    "\n",
    "        iou.append(\n",
    "            MulticlassJaccardIndex(num_classes=151,average='micro',ignore_index=255)\n",
    "            (x1,x2).item()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb6e4484-3eac-4cb3-accf-e86785a276a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45564204835228156"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 518x518\n",
    "np.mean(iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91cb5961-db36-4c18-b889-7de0f91bb550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4499563585400756"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 448x448\n",
    "np.mean(iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3db6a808-0892-4236-b18d-a41586d8ffff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40693133881329413"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 224x224\n",
    "np.mean(iou)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
