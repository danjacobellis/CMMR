{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5932a2e6-c54a-4a4e-bd40-361eced53a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-22 08:45:03.330829: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-22 08:45:03.330879: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-22 08:45:03.330917: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from compressai.models import CompressionModel\n",
    "from compressai.entropy_models import EntropyBottleneck\n",
    "from compressai.layers import GDN1\n",
    "import PIL\n",
    "from torchvision.transforms import ToPILImage\n",
    "from IPython.display import display\n",
    "import zlib\n",
    "from evaluate import evaluator\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fea8a0f6-1fac-43ea-94e1-944002c631ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"checkpoint_dino_rdae_215.pth\", map_location={'cuda:0': 'cpu'})\n",
    "# checkpoint = torch.load(\"checkpoint_dino_rdae_215.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fac7c657-86eb-4a15-b7ac-2503425fddfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ca1412b32242698456339cc8798afc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4ca2ed5e5b4ea4b109e255a0aa08eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imagenet_valid = load_dataset(\"danjacobellis/imagenet_dino\",split='validation').with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d11b961b-8b08-463e-8fd3-50f51cc597d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(in_channels, out_channels, kernel_size=5, stride=2, groups=32):\n",
    "    return nn.Conv2d(\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=kernel_size,\n",
    "        stride=stride,\n",
    "        padding=kernel_size // 2,\n",
    "        groups=groups\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a03b7a65-f4e0-4458-844a-0a7d4501450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv(in_channels, out_channels, kernel_size=5, stride=2, groups=32):\n",
    "    return nn.ConvTranspose2d(\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size=kernel_size,\n",
    "        stride=stride,\n",
    "        output_padding=stride - 1,\n",
    "        padding=kernel_size // 2,\n",
    "        groups=groups\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "223425d0-4085-40ad-9a2c-fe6b3d685d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RateDistortionAutoEncoder(CompressionModel):\n",
    "    def __init__(self, N=4096):\n",
    "        super().__init__()\n",
    "        self.entropy_bottleneck = EntropyBottleneck(N)\n",
    "        self.encode = nn.Sequential(\n",
    "            conv(1536, N, kernel_size=1, stride=1),\n",
    "            GDN1(N),\n",
    "            conv(N, N, kernel_size=5, stride=2),\n",
    "            # GDN1(N),\n",
    "            # conv(N, N, kernel_size=5, stride=2),\n",
    "        )\n",
    "\n",
    "        self.decode = nn.Sequential(\n",
    "            # deconv(N, N, kernel_size=5, stride=2),\n",
    "            # GDN1(N, inverse=True),\n",
    "            deconv(N, N, kernel_size=5, stride=2),\n",
    "            GDN1(N, inverse=True),\n",
    "            deconv(N, 1536, kernel_size=1, stride=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.encode(x)\n",
    "        y_hat, y_likelihoods = self.entropy_bottleneck(y)\n",
    "        x_hat = self.decode(y_hat)\n",
    "        return x_hat, y_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07952794-2d07-4bc7-9b9f-0169fcb43661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = RateDistortionAutoEncoder()\n",
    "net.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f282929-fdf3-465c-9773-0dd537760d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossy_compress_patch_tokens(sample):\n",
    "    with torch.no_grad():\n",
    "        x = sample['patch_tokens']\n",
    "        xr = x.reshape((1,16,16,1536)).permute((0,3,1,2))\n",
    "        z = net.encode(xr)\n",
    "        z = z.round()\n",
    "        z = z.clamp(-128,127)\n",
    "        z = z.to(torch.int8)\n",
    "        z = z.numpy()\n",
    "        original_shape = z.shape\n",
    "        compressed = zlib.compress(z.tobytes(), level=9)\n",
    "        decompressed = zlib.decompress(compressed)\n",
    "        ẑ = np.frombuffer(decompressed, dtype=np.int8)\n",
    "        ẑ = ẑ.reshape(original_shape)\n",
    "        ẑ = torch.tensor(ẑ)\n",
    "        ẑ = ẑ.to(torch.float)\n",
    "        x̂ = net.decode(ẑ)\n",
    "        x̂ = x̂.permute((0, 2, 3, 1)).reshape((1, 256, 1536))\n",
    "        sample['patch_tokens'] = x̂\n",
    "        sample['bps'] = 8*len(compressed)/(1536*16*16)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4762953-ddb3-4deb-a1be-ef5b348690ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet_valid = imagenet_valid.map(lossy_compress_patch_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53df701c-6d99-4c1e-88ea-1cdc52fb8586",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForImageClassification.from_pretrained('facebook/dinov2-giant-imagenet1k-1-layer').to(\"cuda\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2167df2-29a8-4657-a818-8ddd051f30d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = AutoModelForImageClassification.from_pretrained(\"microsoft/resnet-50\")\n",
    "label2id = resnet.config.label2id\n",
    "del resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44922d5d-6365-48c4-bd93-50cf6918e8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.8 s, sys: 1.56 s, total: 51.4 s\n",
      "Wall time: 28.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "n_correct = 0\n",
    "for sample in imagenet_valid:\n",
    "    label = sample['label']\n",
    "    cls_token = sample['cls_token']\n",
    "    patch_tokens = sample['patch_tokens']\n",
    "    linear_input = torch.cat([cls_token, patch_tokens.mean(dim=1)], dim=1).to(\"cuda\")\n",
    "    y = model.classifier(linear_input)[0].argmax().cpu()\n",
    "    if (label == y):\n",
    "        n_correct+=1;\n",
    "acc = n_correct/imagenet_valid.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50406b5f-18c7-4e2f-a42a-f28b1cfc5cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.865"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
